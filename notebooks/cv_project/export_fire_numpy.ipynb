{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP3kplA3ANK4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eFWoTCkCzmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0076b8-6d3b-4164-ca48-f96b7ff58d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfdPVgkSC3jA"
      },
      "outputs": [],
      "source": [
        "file_pattern_train = '/content/drive/My Drive/mit/cv_project/Next_Day_Wildfire_Spread/next_day_wildfire_spread_train*'\n",
        "file_pattern_eval = '/content/drive/My Drive/mit/cv_project/Next_Day_Wildfire_Spread/next_day_wildfire_spread_eval*'\n",
        "file_pattern_test = '/content/drive/My Drive/mit/cv_project/Next_Day_Wildfire_Spread/next_day_wildfire_spread_test*'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utJDKKagAlPq"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cL8C2kQAUfM"
      },
      "outputs": [],
      "source": [
        "\"\"\"Constants for the data reader.\"\"\"\n",
        "\n",
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph' ,\n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask', ]\n",
        "\n",
        "\n",
        "DATA_STATS = {\n",
        "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
        "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
        "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),\n",
        "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
        "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
        "    'th': (0., 360.0, 190.32976, 72.59854),\n",
        "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
        "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
        "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
        "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
        "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
        "    'PrevFireMask': (-1., 1., 0., 1.),\n",
        "    'FireMask': (-1., 1., 0., 1.)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1mvuS5-AmuL"
      },
      "source": [
        "## Preprocess Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlrB-lq3AdKb"
      },
      "outputs": [],
      "source": [
        "\"\"\"Library of common functions used in deep learning neural networks.\n",
        "\"\"\"\n",
        "def random_crop_input_and_output_images(\n",
        "    input_img: tf.Tensor,\n",
        "    output_img: tf.Tensor,\n",
        "    sample_size: int,\n",
        "    num_in_channels: int,\n",
        "    num_out_channels: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Randomly axis-align crop input and output image tensors.\n",
        "\n",
        "  Args:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "    sample_size: side length (square) to crop to.\n",
        "    num_in_channels: number of channels in input_img.\n",
        "    num_out_channels: number of channels in output_img.\n",
        "  Returns:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "  \"\"\"\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(\n",
        "      combined,\n",
        "      [sample_size, sample_size, num_in_channels + num_out_channels],\n",
        "      seed=123)\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def center_crop_input_and_output_images(\n",
        "    input_img: tf.Tensor,\n",
        "    output_img: tf.Tensor,\n",
        "    sample_size: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Center crops input and output image tensors.\n",
        "\n",
        "  Args:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "    sample_size: side length (square) to crop to.\n",
        "  Returns:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "  \"\"\"\n",
        "  central_fraction = sample_size / input_img.shape[0]\n",
        "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
        "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
        "  return input_img, output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGgroE2PAjxM"
      },
      "outputs": [],
      "source": [
        "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
        "\n",
        "def _get_base_key(key: Text) -> Text:\n",
        "  \"\"\"Extracts the base key from the provided key.\n",
        "\n",
        "  Earth Engine exports TFRecords containing each data variable with its\n",
        "  corresponding variable name. In the case of time sequences, the name of the\n",
        "  data variable is of the form 'variable_1', 'v ariable_2', ..., 'variable_n',\n",
        "  where 'variable' is the name of the variable, and n the number of elements\n",
        "  in the time sequence. Extracting the base key ensures that each step of the\n",
        "  time sequence goes through the same normalization steps.\n",
        "  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
        "  For instance, for an input key 'variable_1', this function returns 'variable'.\n",
        "  For an input key 'variable', this function simply returns 'variable'.\n",
        "\n",
        "  Args:\n",
        "    key: Input key.\n",
        "\n",
        "  Returns:\n",
        "    The corresponding base key.\n",
        "\n",
        "  Raises:\n",
        "    ValueError when `key` does not match the expected pattern.\n",
        "  \"\"\"\n",
        "  match = re.match(r'([a-zA-Z]+)', key)\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  raise ValueError(\n",
        "      'The provided key does not match the expected pattern: {}'.format(key))\n",
        "\n",
        "\n",
        "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
        "\n",
        "  Args:\n",
        "    inputs: Inputs to clip and rescale.\n",
        "    key: Key describing the inputs.\n",
        "\n",
        "  Returns:\n",
        "    Clipped and rescaled input.\n",
        "\n",
        "  Raises:\n",
        "    ValueError if there are no data statistics available for `key`.\n",
        "  \"\"\"\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
        "\n",
        "\n",
        "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
        "\n",
        "  Args:\n",
        "    inputs: Inputs to clip and normalize.\n",
        "    key: Key describing the inputs.\n",
        "\n",
        "  Returns:\n",
        "    Clipped and normalized input.\n",
        "\n",
        "  Raises:\n",
        "    ValueError if there are no data statistics available for `key`.\n",
        "  \"\"\"\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  inputs = inputs - mean\n",
        "  return tf.math.divide_no_nan(inputs, std)\n",
        "\n",
        "def _get_features_dict(\n",
        "    sample_size: int,\n",
        "    features: List[Text],\n",
        ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
        "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
        "\n",
        "  Args:\n",
        "    sample_size: Size of the input tiles (square).\n",
        "    features: List of feature names.\n",
        "\n",
        "  Returns:\n",
        "    A features dictionary for TensorFlow IO.\n",
        "  \"\"\"\n",
        "  sample_shape = [sample_size, sample_size]\n",
        "  features = set(features)\n",
        "  columns = [\n",
        "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
        "      for _ in features\n",
        "  ]\n",
        "  return dict(zip(features, columns))\n",
        "\n",
        "\n",
        "# def get_neighbours(inputs, labels):\n",
        "#     bs, w, h, c = inputs.shape\n",
        "#     output_shape = (bs, w-2, h-2, c * 9)\n",
        "#     transformed_pixels = np.zeros(output_shape, dtype=inputs.dtype)\n",
        "\n",
        "#     for i in range(1,w-1):\n",
        "#         for j in range(1, h-1):\n",
        "#             neighborhood = inputs[:, i-1:i+2, j-1:j+2, :]\n",
        "#             transformed_pixels[:, i-1, j-1, :] = neighborhood.reshape(bs, 1, 1, -1)\n",
        "\n",
        "#     return transformed_pixels, labels\n",
        "\n",
        "def get_neighbours(inputs, labels):\n",
        "  w, h, c = inputs.shape\n",
        "  print(type(inputs))\n",
        "  output_shape = (w-2, h-2, c * 9)\n",
        "  transformed_pixels = np.zeros(output_shape, dtype=inputs.dtype)\n",
        "\n",
        "  for i in range(1,w-1):\n",
        "      for j in range(1, h-1):\n",
        "          neighborhood = inputs[i-1:i+2, j-1:j+2, :]\n",
        "          transformed_pixels[i-1, j-1, :] = neighborhood.reshape(1, 1, -1)\n",
        "\n",
        "  return transformed_pixels, labels\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_neighbours_tf(inputs, labels):\n",
        "    if len(inputs.shape) == 3:\n",
        "        inputs = tf.expand_dims(inputs, axis=0)\n",
        "        batch_added = True\n",
        "    else:\n",
        "        batch_added = False\n",
        "\n",
        "    batch_size, w, h, c = inputs.shape\n",
        "    padded_inputs = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')\n",
        "    neighborhoods = tf.image.extract_patches(\n",
        "        images=padded_inputs,\n",
        "        sizes=[1, 3, 3, 1],\n",
        "        strides=[1, 1, 1, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "\n",
        "    output_shape = [batch_size, neighborhoods.shape[1], neighborhoods.shape[2], 3*3*c]\n",
        "    transformed_pixels = tf.reshape(neighborhoods, output_shape)\n",
        "\n",
        "    if batch_added:\n",
        "        transformed_pixels = tf.squeeze(transformed_pixels, axis=0)\n",
        "\n",
        "    return transformed_pixels, labels\n",
        "\n",
        "\n",
        "\n",
        "def _parse_fn(\n",
        "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
        "    num_in_channels: int, clip_and_normalize: bool,\n",
        "    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
        "    with_neighbours\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Reads a serialized example.\n",
        "\n",
        "  Args:\n",
        "    example_proto: A TensorFlow example protobuf.\n",
        "    data_size: Size of tiles (square) as read from input files.\n",
        "    sample_size: Size the tiles (square) when input into the model.\n",
        "    num_in_channels: Number of input channels.\n",
        "    clip_and_normalize: True if the data should be clipped and normalized.\n",
        "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
        "    random_crop: True if the data should be randomly cropped.\n",
        "    center_crop: True if the data should be cropped in the center.\n",
        "    get_neighbourds: True if each pixel should be replaced by the array\n",
        "      of the full square of neighbourds (himself+8, starting left->right, top->bot)\n",
        "\n",
        "  Returns:\n",
        "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
        "  \"\"\"\n",
        "  if (random_crop and center_crop):\n",
        "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
        "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
        "  feature_names = input_features + output_features\n",
        "  features_dict = _get_features_dict(data_size, feature_names)\n",
        "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "\n",
        "  if clip_and_normalize:\n",
        "    inputs_list = [\n",
        "        _clip_and_normalize(features.get(key), key) for key in input_features\n",
        "    ]\n",
        "  elif clip_and_rescale:\n",
        "    inputs_list = [\n",
        "        _clip_and_rescale(features.get(key), key) for key in input_features\n",
        "    ]\n",
        "  else:\n",
        "    inputs_list = [features.get(key) for key in input_features]\n",
        "\n",
        "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
        "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
        "\n",
        "  outputs_list = [features.get(key) for key in output_features]\n",
        "  assert outputs_list, 'outputs_list should not be empty'\n",
        "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
        "\n",
        "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
        "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
        "                                            'but dimensions of outputs_stacked'\n",
        "                                            f' are {outputs_stacked_shape}')\n",
        "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
        "\n",
        "  if random_crop:\n",
        "    input_img, output_img = random_crop_input_and_output_images(\n",
        "        input_img, output_img, sample_size, num_in_channels, 1)\n",
        "  if center_crop:\n",
        "    input_img, output_img = center_crop_input_and_output_images(\n",
        "        input_img, output_img, sample_size)\n",
        "  if with_neighbours:\n",
        "    input_img, output_img = get_neighbours_tf(input_img, output_img)\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
        "                batch_size: int, num_in_channels: int, compression_type: Text,\n",
        "                clip_and_normalize: bool, clip_and_rescale: bool,\n",
        "                random_crop: bool, center_crop: bool, with_neighbours:bool) -> tf.data.Dataset:\n",
        "  \"\"\"Gets the dataset from the file pattern.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: Input file pattern.\n",
        "    data_size: Size of tiles (square) as read from input files.\n",
        "    sample_size: Size the tiles (square) when input into the model.\n",
        "    batch_size: Batch size.\n",
        "    num_in_channels: Number of input channels.\n",
        "    compression_type: Type of compression used for the input files.\n",
        "    clip_and_normalize: True if the data should be clipped and normalized, False\n",
        "      otherwise.\n",
        "    clip_and_rescale: True if the data should be clipped and rescaled, False\n",
        "      otherwise.\n",
        "    random_crop: True if the data should be randomly cropped.\n",
        "    center_crop: True if the data shoulde be cropped in the center.\n",
        "    with_neighbours\n",
        "  Returns:\n",
        "    A TensorFlow dataset loaded from the input file pattern, with features\n",
        "    described in the constants, and with the shapes determined from the input\n",
        "    parameters to this function.\n",
        "  \"\"\"\n",
        "  if (clip_and_normalize and clip_and_rescale):\n",
        "    raise ValueError('Cannot have both normalize and rescale.')\n",
        "  dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n",
        "  dataset = dataset.interleave(\n",
        "      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.map(\n",
        "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
        "          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
        "          clip_and_rescale, random_crop, center_crop, with_neighbours),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=np.True_)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IffIhWuFm3vO"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyExaHPNSYpS"
      },
      "outputs": [],
      "source": [
        "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_inter_op_parallelism_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZa8Y29ZV7ab"
      },
      "outputs": [],
      "source": [
        "side_size = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
        "batch_size = 100 #batch size\n",
        "\n",
        "training_dataset = get_dataset(\n",
        "      file_pattern_train,\n",
        "      data_size=64,\n",
        "      sample_size=side_size, #sample_size is such a bad name\n",
        "      batch_size=batch_size,\n",
        "      num_in_channels=12,\n",
        "      compression_type=None,\n",
        "      clip_and_normalize=False,\n",
        "      clip_and_rescale=False,\n",
        "      random_crop=True,\n",
        "      center_crop=False,\n",
        "      with_neighbours=False)\n",
        "\n",
        "eval_dataset = get_dataset(\n",
        "      file_pattern_eval,\n",
        "      data_size=64,\n",
        "      sample_size=side_size,\n",
        "      batch_size=batch_size,\n",
        "      num_in_channels=12,\n",
        "      compression_type=None,\n",
        "      clip_and_normalize=False,\n",
        "      clip_and_rescale=False,\n",
        "      random_crop=True,\n",
        "      center_crop=False,\n",
        "      with_neighbours=False)\n",
        "\n",
        "testing_dataset = get_dataset(\n",
        "      file_pattern_test,\n",
        "      data_size=64,\n",
        "      sample_size=side_size,\n",
        "      batch_size=batch_size,\n",
        "      num_in_channels=12,\n",
        "      compression_type=None,\n",
        "      clip_and_normalize=False,\n",
        "      clip_and_rescale=False,\n",
        "      random_crop=True,\n",
        "      center_crop=False,\n",
        "      with_neighbours=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_np_array(dataset):\n",
        "  inputs = []\n",
        "  outputs = []\n",
        "  for input_batch, output_batch in dataset:\n",
        "      inputs.append(input_batch.numpy())\n",
        "      outputs.append(output_batch.numpy())\n",
        "  full_inputs_array = np.concatenate(inputs, axis=0)\n",
        "  full_outputs_array = np.concatenate(outputs, axis=0)\n",
        "  return full_inputs_array, full_outputs_array\n",
        "ti, to = get_np_array(training_dataset)\n",
        "ei, eo = get_np_array(eval_dataset)\n",
        "tei, teo = get_np_array(testing_dataset)"
      ],
      "metadata": {
        "id": "K-0BxgU3Hf1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZbzNwrGm19_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ti.shape[0], ei.shape[0], tei.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8Y9ipwN1-Wk",
        "outputId": "d64e8fe2-5bd4-40f7-8235-8ba890414fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14979, 1877, 1689)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/content/drive/MyDrive/mit/cv_project/data/raw'\n",
        "np.save(os.path.join(path, 'train_features.npy'), ti)\n",
        "np.save(os.path.join(path, 'train_labels.npy'), to)\n",
        "np.save(os.path.join(path, 'val_features.npy'), ei)\n",
        "np.save(os.path.join(path, 'val_labels.npy'), eo)\n",
        "np.save(os.path.join(path, 'test_features.npy'), tei)\n",
        "np.save(os.path.join(path, 'test_labels.npy'), teo)"
      ],
      "metadata": {
        "id": "n-74qJtaJNSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}